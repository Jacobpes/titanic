{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "import numpy as np # type: ignore\n",
    "\n",
    "# Dataframe operations\n",
    "import pandas as pd # type: ignore\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import MinMaxScaler # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler # type: ignore\n",
    "from sklearn.utils import shuffle # type: ignore\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression # type: ignore\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest # type: ignore\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN # type: ignore\n",
    "from sklearn.neural_network import MLPClassifier # type: ignore\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes # type: ignore\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree # type: ignore\n",
    "from sklearn import svm #support vector Machine # type: ignore\n",
    "from sklearn.linear_model import Perceptron # type: ignore\n",
    "from sklearn.ensemble import GradientBoostingClassifier # type: ignore\n",
    "\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix # type: ignore\n",
    "from sklearn.model_selection import train_test_split #training and testing data split# type: ignore\n",
    "from sklearn import metrics #accuracy measure # type: ignore\n",
    "from sklearn.ensemble import VotingClassifier # type: ignore\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation # type: ignore\n",
    "from sklearn.model_selection import cross_val_score #score evaluation # type: ignore\n",
    "from sklearn.model_selection import cross_val_predict #prediction # type: ignore\n",
    "from sklearn.model_selection import cross_validate # type: ignore\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV # type: ignore\n",
    "\n",
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process # type: ignore\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder # type: ignore\n",
    "from sklearn import feature_selection # type: ignore\n",
    "from sklearn import model_selection # type: ignore\n",
    "from sklearn import metrics # type: ignore\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import matplotlib.pylab as pylab # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "from pandas.plotting import scatter_matrix # type: ignore\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder # type: ignore\n",
    "from sklearn.preprocessing import MinMaxScaler # type: ignore\n",
    "from sklearn.metrics import accuracy_score # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "\n",
    "from sklearn.impute import SimpleImputer # type: ignore\n",
    "from sklearn.svm import SVC # type: ignore\n",
    "import os # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  1309 non-null   int64  \n",
      " 1   Survived     891 non-null    float64\n",
      " 2   Pclass       1309 non-null   int64  \n",
      " 3   Name         1309 non-null   object \n",
      " 4   Sex          1309 non-null   object \n",
      " 5   Age          1046 non-null   float64\n",
      " 6   SibSp        1309 non-null   int64  \n",
      " 7   Parch        1309 non-null   int64  \n",
      " 8   Ticket       1309 non-null   object \n",
      " 9   Fare         1308 non-null   float64\n",
      " 10  Cabin        295 non-null    object \n",
      " 11  Embarked     1307 non-null   object \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 122.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "data_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "data_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId       0\n",
      "Survived        418\n",
      "Pclass            0\n",
      "Name              0\n",
      "Sex               0\n",
      "Age               0\n",
      "SibSp             0\n",
      "Parch             0\n",
      "Ticket            0\n",
      "Fare              1\n",
      "Cabin          1014\n",
      "Embarked          2\n",
      "Family_Size       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract Title from Name\n",
    "data_df['Title'] = data_df['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
    "\n",
    "# Map rare titles to more common ones\n",
    "mapping = {\n",
    "    'Mlle': 'Miss', 'Mme': 'Mrs', 'Ms': 'Miss', 'Dr': 'Dr',\n",
    "    'Major': 'Mr', 'Lady': 'Mrs', 'Sir': 'Mr', 'Col': 'Mr',\n",
    "    'Capt': 'Mr', 'Countess': 'Mrs', 'Jonkheer': 'Mr',\n",
    "    'Dona': 'Mrs', 'Don': 'Mr', 'Rev': 'Rev', 'Master': 'Master',\n",
    "    'Miss': 'Miss', 'Mr': 'Mr', 'Mrs': 'Mrs'\n",
    "}\n",
    "data_df['Title'] = data_df['Title'].map(mapping)\n",
    "\n",
    "# Fill missing Age based on Title median\n",
    "age_by_title = data_df.groupby('Title')['Age'].median()\n",
    "data_df['Age'] = data_df.apply(lambda row: age_by_title[row['Title']] if pd.isnull(row['Age']) else row['Age'], axis=1)\n",
    "\n",
    "# Calculate Family_Size and re-split data_df using .loc to avoid SettingWithCopyWarning\n",
    "data_df['Family_Size'] = data_df['Parch'] + data_df['SibSp']\n",
    "\n",
    "data_df.drop('Title', axis=1, inplace=True)\n",
    "\n",
    "train_df = data_df.loc[:890].copy()\n",
    "test_df = data_df.loc[891:].copy()\n",
    "\n",
    "# Check for missing values in the dataset\n",
    "missing_values = data_df.isnull().sum()\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passengers with family survival information: 420\n"
     ]
    }
   ],
   "source": [
    "data_df['Last_Name'] = data_df['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "data_df['Fare'] = data_df['Fare'].fillna(data_df['Fare'].median())\n",
    "\n",
    "DEFAULT_SURVIVAL_VALUE = 0.25\n",
    "data_df['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n",
    "\n",
    "for grp, grp_df in data_df[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n",
    "                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n",
    "    \n",
    "    if (len(grp_df) != 1):\n",
    "        # A Family group is found.\n",
    "        for ind, row in grp_df.iterrows():\n",
    "            smax = grp_df.drop(ind)['Survived'].max()\n",
    "            smin = grp_df.drop(ind)['Survived'].min()\n",
    "            passID = row['PassengerId']\n",
    "            if (smax == 1):\n",
    "                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "            elif (smin==0):\n",
    "                data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "\n",
    "tolerance = 1e-10\n",
    "filtered_data_df = data_df[abs(data_df['Family_Survival'] - 0.25) > tolerance]\n",
    "print(\"Number of passengers with family survival information:\", filtered_data_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passengers with family/group survival information: 546\n"
     ]
    }
   ],
   "source": [
    "for _, grp_df in data_df.groupby('Ticket'):\n",
    "    if (len(grp_df) != 1):\n",
    "        for ind, row in grp_df.iterrows():\n",
    "            if (row['Family_Survival'] == 0) | (np.isclose(row['Family_Survival'], 0.25, rtol=1e-09, atol=1e-09)):\n",
    "                smax = grp_df.drop(ind)['Survived'].max()\n",
    "                smin = grp_df.drop(ind)['Survived'].min()\n",
    "                passID = row['PassengerId']\n",
    "                if (smax == 1):\n",
    "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "                elif (smin==0):\n",
    "                    data_df.loc[data_df['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "                        \n",
    "# Correcting the line to avoid the ValueError\n",
    "print(\"Number of passengers with family/group survival information:\", \n",
    "      data_df.loc[~np.isclose(data_df['Family_Survival'], 0.25, rtol=1e-09, atol=1e-09)].shape[0])\n",
    "\n",
    "train_df = data_df.loc[:890].copy()\n",
    "test_df = data_df.loc[891:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1       0.0       3   \n",
      "1            2       1.0       1   \n",
      "2            3       1.0       3   \n",
      "3            4       1.0       1   \n",
      "4            5       0.0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket Cabin Embarked  Family_Size  Last_Name  \\\n",
      "0      0         A/5 21171   NaN        S            1     Braund   \n",
      "1      0          PC 17599   C85        C            1    Cumings   \n",
      "2      0  STON/O2. 3101282   NaN        S            0  Heikkinen   \n",
      "3      0            113803  C123        S            1   Futrelle   \n",
      "4      0            373450   NaN        S            0      Allen   \n",
      "\n",
      "   Family_Survival  FareBin_Code  \n",
      "0             0.25             0  \n",
      "1             0.25             4  \n",
      "2             0.25             1  \n",
      "3             0.00             4  \n",
      "4             0.25             1  \n"
     ]
    }
   ],
   "source": [
    "# Use the assignment directly instead of inplace=True\n",
    "data_df['Fare'] = data_df['Fare'].fillna(data_df['Fare'].median())\n",
    "\n",
    "# Making Bins\n",
    "data_df['FareBin'] = pd.qcut(data_df['Fare'], 5)\n",
    "\n",
    "label = LabelEncoder()\n",
    "data_df['FareBin_Code'] = label.fit_transform(data_df['FareBin'])\n",
    "\n",
    "# Drop the original 'Fare' column as it's now represented by 'FareBin_Code'\n",
    "data_df.drop(columns=['Fare', 'FareBin'], inplace=True)\n",
    "\n",
    "train_df = data_df.loc[:890].copy()\n",
    "test_df = data_df.loc[891:].copy()\n",
    "\n",
    "# Display the first few rows of the updated training DataFrame to verify the changes\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping Age in 4 Bins\n",
    "data_df['AgeBin'] = pd.qcut(data_df['Age'], 4)\n",
    "\n",
    "label = LabelEncoder()\n",
    "data_df['AgeBin_Code'] = label.fit_transform(data_df['AgeBin'])\n",
    "\n",
    "data_df.drop(columns=['AgeBin', 'Age'], inplace=True)\n",
    "\n",
    "train_df = data_df.loc[:890].copy()\n",
    "test_df = data_df.loc[891:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Last_Name</th>\n",
       "      <th>Family_Survival</th>\n",
       "      <th>FareBin_Code</th>\n",
       "      <th>AgeBin_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>0</td>\n",
       "      <td>Kelly</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>Wilkes</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>0</td>\n",
       "      <td>Myles</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>Wirz</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>Hirvonen</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "891          892       NaN       3   \n",
       "892          893       NaN       3   \n",
       "893          894       NaN       2   \n",
       "894          895       NaN       3   \n",
       "895          896       NaN       3   \n",
       "\n",
       "                                             Name     Sex  SibSp  Parch  \\\n",
       "891                              Kelly, Mr. James    male      0      0   \n",
       "892              Wilkes, Mrs. James (Ellen Needs)  female      1      0   \n",
       "893                     Myles, Mr. Thomas Francis    male      0      0   \n",
       "894                              Wirz, Mr. Albert    male      0      0   \n",
       "895  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female      1      1   \n",
       "\n",
       "      Ticket Cabin Embarked  Family_Size Last_Name  Family_Survival  \\\n",
       "891   330911   NaN        Q            0     Kelly             0.25   \n",
       "892   363272   NaN        S            1    Wilkes             0.25   \n",
       "893   240276   NaN        Q            0     Myles             0.25   \n",
       "894   315154   NaN        S            0      Wirz             0.25   \n",
       "895  3101298   NaN        S            2  Hirvonen             1.00   \n",
       "\n",
       "     FareBin_Code  AgeBin_Code  \n",
       "891             0            2  \n",
       "892             0            3  \n",
       "893             1            3  \n",
       "894             1            1  \n",
       "895             2            0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1       0.0       3   \n",
      "1            2       1.0       1   \n",
      "2            3       1.0       3   \n",
      "\n",
      "                                                Name  Sex            Ticket  \\\n",
      "0                            Braund, Mr. Owen Harris    1         A/5 21171   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0          PC 17599   \n",
      "2                             Heikkinen, Miss. Laina    0  STON/O2. 3101282   \n",
      "\n",
      "   Cabin Embarked  Family_Size  Family_Survival  FareBin_Code  AgeBin_Code  \n",
      "0      1        S            1             0.25             0            0  \n",
      "1      0        C            1             0.25             4            3  \n",
      "2      1        S            0             0.25             1            1  \n",
      "     PassengerId  Survived  Pclass                              Name  Sex  \\\n",
      "891          892       NaN       3                  Kelly, Mr. James    1   \n",
      "892          893       NaN       3  Wilkes, Mrs. James (Ellen Needs)    0   \n",
      "893          894       NaN       2         Myles, Mr. Thomas Francis    1   \n",
      "\n",
      "     Ticket  Cabin Embarked  Family_Size  Family_Survival  FareBin_Code  \\\n",
      "891  330911      1        Q            0             0.25             0   \n",
      "892  363272      1        S            1             0.25             0   \n",
      "893  240276      1        Q            0             0.25             1   \n",
      "\n",
      "     AgeBin_Code  \n",
      "891            2  \n",
      "892            3  \n",
      "893            3  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "sex_mapping = {'female':0, 'male':1}\n",
    "\n",
    "data_df['Sex'] = data_df['Sex'].map(sex_mapping).astype(int)\n",
    "\n",
    "# Set 'Cabin' to 0 if it's NaN, else set it to 1\n",
    "data_df['Cabin'] = data_df['Cabin'].isna().astype(int)\n",
    "\n",
    "# Drop rows where 'Sex' is NaN\n",
    "data_df.dropna(subset=['Sex'], inplace=True)\n",
    "\n",
    "# Drop unnecessary columns from both datasets\n",
    "columns_to_drop = ['SibSp', 'Parch', 'Last_Name']\n",
    "data_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "train_df = data_df.loc[:890].copy()\n",
    "test_df = data_df.loc[891:].copy()\n",
    "\n",
    "# Display the first few rows of the training dataframe\n",
    "print(train_df.head(3))\n",
    "print(test_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "891          892       NaN       3   \n",
      "892          893       NaN       3   \n",
      "893          894       NaN       2   \n",
      "894          895       NaN       3   \n",
      "895          896       NaN       3   \n",
      "\n",
      "                                             Name  Sex   Ticket  Cabin  \\\n",
      "891                              Kelly, Mr. James    1   330911      1   \n",
      "892              Wilkes, Mrs. James (Ellen Needs)    0   363272      1   \n",
      "893                     Myles, Mr. Thomas Francis    1   240276      1   \n",
      "894                              Wirz, Mr. Albert    1   315154      1   \n",
      "895  Hirvonen, Mrs. Alexander (Helga E Lindqvist)    0  3101298      1   \n",
      "\n",
      "     Embarked  Family_Size  Family_Survival  FareBin_Code  AgeBin_Code  \n",
      "891         2            0             0.25             0            2  \n",
      "892         0            1             0.25             0            3  \n",
      "893         2            0             0.25             1            3  \n",
      "894         0            0             0.25             1            1  \n",
      "895         0            2             1.00             2            0  \n"
     ]
    }
   ],
   "source": [
    "# If 'Embarked' has missing values, fill them with the most common value\n",
    "if data_df['Embarked'].isna().sum() > 0:\n",
    "    common_value = data_df['Embarked'].mode()[0]\n",
    "    data_df['Embarked'] = data_df['Embarked'].fillna(common_value)\n",
    "\n",
    "# Define a dictionary to map 'Embarked' values to integers\n",
    "embarked_mapping = {'S': 0, 'C': 1, 'Q': 2}\n",
    "\n",
    "# Use the map method to convert 'Embarked' values to integers\n",
    "data_df['Embarked'] = data_df['Embarked'].map(embarked_mapping).astype(int)\n",
    "\n",
    "train_df = data_df.loc[:890].copy()\n",
    "test_df = data_df.loc[891:].copy()\n",
    "\n",
    "print(test_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1       0.0       3   \n",
      "1            2       1.0       1   \n",
      "2            3       1.0       3   \n",
      "\n",
      "                                                Name  Sex  Ticket  Cabin  \\\n",
      "0                            Braund, Mr. Owen Harris    1       0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0       0      0   \n",
      "2                             Heikkinen, Miss. Laina    0       0      1   \n",
      "\n",
      "   Embarked  Family_Size  Family_Survival  FareBin_Code  AgeBin_Code  \n",
      "0         0            1             0.25             0            0  \n",
      "1         1            1             0.25             4            3  \n",
      "2         0            0             0.25             1            1  \n"
     ]
    }
   ],
   "source": [
    "data_df['Ticket'] = data_df['Ticket'].apply(lambda x: 1 if x.isnumeric() else 0)\n",
    "\n",
    "train_df = data_df.loc[:890].copy()\n",
    "test_df = data_df.loc[891:].copy()\n",
    "print(train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  Name  Sex  Ticket  Cabin  Embarked  \\\n",
      "0            1       0.0       3     1    1       0      1         0   \n",
      "1            2       1.0       1     0    0       0      0         1   \n",
      "2            3       1.0       3     0    0       0      1         0   \n",
      "3            4       1.0       1     0    0       1      0         0   \n",
      "4            5       0.0       3     1    1       1      1         0   \n",
      "\n",
      "   Family_Size  Family_Survival  FareBin_Code  AgeBin_Code  \n",
      "0            1             0.25             0            0  \n",
      "1            1             0.25             4            3  \n",
      "2            0             0.25             1            1  \n",
      "3            1             0.00             4            2  \n",
      "4            0             0.25             1            2  \n",
      "     PassengerId  Survived  Pclass  Name  Sex  Ticket  Cabin  Embarked  \\\n",
      "891          892       NaN       3     1    1       1      1         2   \n",
      "892          893       NaN       3     0    0       1      1         0   \n",
      "893          894       NaN       2     1    1       1      1         2   \n",
      "894          895       NaN       3     1    1       1      1         0   \n",
      "895          896       NaN       3     0    0       1      1         0   \n",
      "\n",
      "     Family_Size  Family_Survival  FareBin_Code  AgeBin_Code  \n",
      "891            0             0.25             0            2  \n",
      "892            1             0.25             0            3  \n",
      "893            0             0.25             1            3  \n",
      "894            0             0.25             1            1  \n",
      "895            2             1.00             2            0  \n"
     ]
    }
   ],
   "source": [
    "# Set lower surviving rate titles to 1, others to 0\n",
    "data_df['Name'] = data_df['Name'].str.contains('Mr. ').astype(int)\n",
    "\n",
    "# Splitting into train and test DataFrames\n",
    "train_df = data_df.loc[:890].copy()\n",
    "test_df = data_df.loc[891:].copy()\n",
    "\n",
    "# Now your code should work without the IntCastingNaNError\n",
    "print(train_df.head(5))\n",
    "print(test_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (891, 11) y.shape (891,) X_test.shape (418, 11)\n"
     ]
    }
   ],
   "source": [
    "# drop survived from X\n",
    "X = train_df.drop('Survived', axis=1)\n",
    "# y has to have only 'PassengerId', 'Survived' columns\n",
    "y = train_df['Survived'].astype(int)\n",
    "X_test = test_df.copy().drop('Survived', axis=1)\n",
    "print(\"X.shape\", X.shape,\"y.shape\", y.shape, \"X_test.shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X    PassengerId  Pclass  Name  Sex  Ticket  Cabin  Embarked  Family_Size  \\\n",
      "0            1       3     1    1       0      1         0            1   \n",
      "1            2       1     0    0       0      0         1            1   \n",
      "2            3       3     0    0       0      1         0            0   \n",
      "3            4       1     0    0       1      0         0            1   \n",
      "4            5       3     1    1       1      1         0            0   \n",
      "\n",
      "   Family_Survival  FareBin_Code  AgeBin_Code  \n",
      "0             0.25             0            0  \n",
      "1             0.25             4            3  \n",
      "2             0.25             1            1  \n",
      "3             0.00             4            2  \n",
      "4             0.25             1            2  \n",
      "X_test      PassengerId  Pclass  Name  Sex  Ticket  Cabin  Embarked  Family_Size  \\\n",
      "891          892       3     1    1       1      1         2            0   \n",
      "892          893       3     0    0       1      1         0            1   \n",
      "893          894       2     1    1       1      1         2            0   \n",
      "894          895       3     1    1       1      1         0            0   \n",
      "895          896       3     0    0       1      1         0            2   \n",
      "\n",
      "     Family_Survival  FareBin_Code  AgeBin_Code  \n",
      "891             0.25             0            2  \n",
      "892             0.25             0            3  \n",
      "893             0.25             1            3  \n",
      "894             0.25             1            1  \n",
      "895             1.00             2            0  \n"
     ]
    }
   ],
   "source": [
    "print(\"X\", X.head())\n",
    "print(\"X_test\", X_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
      "SVC Best Score: 0.8529463171036206\n",
      "SVC Best Estimator: SVC(C=10, gamma='auto', probability=True)\n",
      "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n",
      "RandomForestClassifier Best Score: 0.8540449438202249\n",
      "RandomForestClassifier Best Estimator: RandomForestClassifier(max_depth=10, min_samples_leaf=2)\n",
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n",
      "KNeighborsClassifier Best Score: 0.8316229712858926\n",
      "KNeighborsClassifier Best Estimator: KNeighborsClassifier(n_neighbors=7, p=1)\n",
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "LogisticRegression Best Score: 0.8316479400749065\n",
      "LogisticRegression Best Estimator: LogisticRegression(C=0.1, solver='liblinear')\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "MLPClassifier Best Score: 0.8204438958028921\n",
      "MLPClassifier Best Estimator: MLPClassifier(activation='logistic', alpha=0.05, hidden_layer_sizes=(90, 90),\n",
      "              learning_rate='adaptive', max_iter=1000)\n",
      "Submission saved successfully!\n",
      "y_pred [0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1\n",
      " 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 1 0 1 1 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters for the SVC\n",
    "svc_hyperparams = {\n",
    "    'C': [0.1, 1, 10, 100], # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'], # Kernel function\n",
    "    'gamma': ['scale', 'auto'], # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "    'class_weight': ['balanced', None] # Weights associated with classes\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV with the SVC estimator\n",
    "svc_gd = GridSearchCV(estimator=SVC(probability=True), param_grid=svc_hyperparams, verbose=True, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "# Fit the model to the scaled training data\n",
    "svc_gd.fit(X_scaled, y)\n",
    "\n",
    "# Print the best score and the best estimator for SVC\n",
    "print(\"SVC Best Score:\", svc_gd.best_score_)\n",
    "print(\"SVC Best Estimator:\", svc_gd.best_estimator_)\n",
    "\n",
    "# ************************************************************************************************************************************************************\n",
    "\n",
    "# Define the hyperparameters for the RandomForestClassifier\n",
    "rf_hyperparams = {\n",
    "    'n_estimators': [100, 200], # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20], # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4] # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV with the RandomForestClassifier estimator\n",
    "rf_gd = GridSearchCV(estimator=RandomForestClassifier(), param_grid=rf_hyperparams, verbose=True, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "# Fit the model to the scaled training data\n",
    "rf_gd.fit(X_scaled, y)\n",
    "\n",
    "# Print the best score and the best estimator for RandomForestClassifier\n",
    "print(\"RandomForestClassifier Best Score:\", rf_gd.best_score_)\n",
    "print(\"RandomForestClassifier Best Estimator:\", rf_gd.best_estimator_)\n",
    "\n",
    "# ************************************************************************************************************************************************************\n",
    "\n",
    "\n",
    "# Define the hyperparameters for the KNeighborsClassifier\n",
    "knn_hyperparams = {\n",
    "    'n_neighbors': [3, 5, 7], # Number of neighbors to use\n",
    "    'weights': ['uniform', 'distance'], # Weight function used in prediction\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], # Algorithm used to compute the nearest neighbors\n",
    "    'p': [1, 2] # Power parameter for the Minkowski metric\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV with the KNeighborsClassifier estimator\n",
    "knn_gd = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=knn_hyperparams, verbose=True, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "# Fit the model to the scaled training data\n",
    "knn_gd.fit(X_scaled, y)\n",
    "\n",
    "# Print the best score and the best estimator for KNeighborsClassifier\n",
    "print(\"KNeighborsClassifier Best Score:\", knn_gd.best_score_)\n",
    "print(\"KNeighborsClassifier Best Estimator:\", knn_gd.best_estimator_)\n",
    "\n",
    "\n",
    "# ************************************************************************************************************************************************************\n",
    "\n",
    "# Define the hyperparameters for the LogisticRegression\n",
    "\n",
    "lr_hyperparams = {\n",
    "    'penalty': ['l2'],  # 'l1' removed due to compatibility issues with the default solver 'lbfgs'\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear'],  # Added 'liblinear' solver for compatibility with 'l1' penalty\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV with the LogisticRegression estimator (updated)\n",
    "lr_gd = GridSearchCV(estimator=LogisticRegression(), param_grid=lr_hyperparams, verbose=True, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "# Fit the model to the scaled training data\n",
    "lr_gd.fit(X_scaled, y)\n",
    "\n",
    "# Print the best score and the best estimator for LogisticRegression\n",
    "print(\"LogisticRegression Best Score:\", lr_gd.best_score_)\n",
    "print(\"LogisticRegression Best Estimator:\", lr_gd.best_estimator_)\n",
    "\n",
    "\n",
    "# ************************************************************************************************************************************************************\n",
    "\n",
    "# Define the hyperparameters for the MLPClassifier\n",
    "mlp_hyperparams = {\n",
    "    'hidden_layer_sizes': [(90, 90)], # The ith element represents the number of neurons in the ith hidden layer\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'], # Activation function for the hidden layer\n",
    "    'solver': ['adam'], # Optimizer for weight optimization\n",
    "    'alpha': [0.05], # L2 penalty (regularization term) parameter\n",
    "    'learning_rate': ['adaptive'], # Learning rate schedule for weight updates\n",
    "    'max_iter': [1000] # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV with the MLPClassifier estimator\n",
    "mlp_gd = GridSearchCV(estimator=MLPClassifier(), param_grid=mlp_hyperparams, verbose=True, cv=2, scoring=\"accuracy\")\n",
    "\n",
    "# Fit the model to the scaled training data\n",
    "mlp_gd.fit(X_scaled, y)\n",
    "\n",
    "# Print the best score and the best estimator for MLPClassifier\n",
    "print(\"MLPClassifier Best Score:\", mlp_gd.best_score_)\n",
    "print(\"MLPClassifier Best Estimator:\", mlp_gd.best_estimator_)\n",
    "\n",
    "# ************************************************************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the VotingClassifier with the best estimators from GridSearchCV\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('best_svc', svc_gd.best_estimator_),\n",
    "    ('best_rf', rf_gd.best_estimator_),\n",
    "    ('best_knn', knn_gd.best_estimator_),\n",
    "    ('best_lr', lr_gd.best_estimator_),\n",
    "    ('best_mlp', mlp_gd.best_estimator_),\n",
    "\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the VotingClassifier to the scaled training data\n",
    "voting_clf.fit(X_scaled, y)\n",
    "\n",
    "# Predict the test data with the VotingClassifier\n",
    "y_pred = voting_clf.predict(X_test_scaled)\n",
    "\n",
    "# Prepare the submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': y_pred\n",
    "})\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = '../data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv(f'{output_dir}/gender_submission.csv', index=False)\n",
    "print(\"Submission saved successfully!\")\n",
    "print(\"y_pred\", y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
